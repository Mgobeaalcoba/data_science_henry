{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# üìö RESUMEN COMPLETO: Data Science & Machine Learning\n",
    "## Clase de Consulta - Repaso Pre-Proyecto Final\n",
    "\n",
    "**Fecha**: Lunes (Semana del 17 de Febrero, 2026)  \n",
    "**Instructor**: Mariano Gobea  \n",
    "**Duraci√≥n**: 2 horas\n",
    "\n",
    "---\n",
    "\n",
    "## üéØ Objetivo de esta Clase\n",
    "\n",
    "Esta clase de consulta tiene como objetivo:\n",
    "1. **Repasar** los conceptos m√°s importantes de cada m√≥dulo\n",
    "2. **Resolver dudas** antes del proyecto final\n",
    "3. **Proporcionar gu√≠as** para abordar el proyecto\n",
    "4. **Compartir mejores pr√°cticas** y errores comunes\n",
    "\n",
    "**No se presentar√° contenido nuevo**. Todo en este notebook es repaso.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üìã TABLA DE CONTENIDOS\n",
    "\n",
    "1. [Estad√≠sticas de lo Aprendido](#stats)\n",
    "2. [M√≥dulo 1: Fundamentos (Clases 01-04)](#modulo1)\n",
    "3. [M√≥dulo 2: Avanzado (Clases 05-06)](#modulo2)\n",
    "4. [M√≥dulo 3: No Supervisado (Clases 07-08)](#modulo3)\n",
    "5. [M√≥dulo 4: Series Temporales y DL (Clases 09-10)](#modulo4)\n",
    "6. [Cheat Sheet de C√≥digo](#cheatsheet)\n",
    "7. [Proyecto Final: Gu√≠a Paso a Paso](#proyecto)\n",
    "8. [Q&A y Consultas](#qa)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='stats'></a>\n",
    "## üìä ESTAD√çSTICAS DE LO APRENDIDO\n",
    "\n",
    "### En 10 clases dominaste:\n",
    "\n",
    "**‚úÖ 25+ Algoritmos:**\n",
    "- 6 de regresi√≥n (Linear, Ridge, Lasso, ElasticNet, Polynomial, + ensemble)\n",
    "- 8 de clasificaci√≥n (LogReg, KNN, DT, RF, SVM, XGBoost, LightGBM, CatBoost)\n",
    "- 3 de clustering (K-Means, DBSCAN, Hierarchical)\n",
    "- 4 de series temporales (ARIMA, SARIMA, Prophet, LSTM)\n",
    "- 2 de Deep Learning (Dense Networks, LSTM)\n",
    "- 2 de recomendaci√≥n (Collaborative, Content-Based)\n",
    "\n",
    "**‚úÖ 20+ M√©tricas:**\n",
    "- Regresi√≥n: MAE, MSE, RMSE, R¬≤, MAPE\n",
    "- Clasificaci√≥n: Accuracy, Precision, Recall, F1, ROC-AUC\n",
    "- Clustering: Silhouette, Davies-Bouldin, Calinski-Harabasz\n",
    "- Recomendaciones: Precision@K, Recall@K, NDCG@K\n",
    "\n",
    "**‚úÖ 20 Datasets Reales:**\n",
    "- Retail, Finanzas, Fintech, E-commerce, Movilidad\n",
    "\n",
    "**‚úÖ 23 Notebooks Ejecutados:**\n",
    "- ~15,000 l√≠neas de c√≥digo\n",
    "- Casos pr√°cticos de negocio\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='modulo1'></a>\n",
    "## üìñ M√ìDULO 1: FUNDAMENTOS (Clases 01-04)\n",
    "\n",
    "### **Clase 01: Introducci√≥n al ML**\n",
    "\n",
    "#### Conceptos Fundamentales\n",
    "\n",
    "**Jerarqu√≠a de t√©rminos:**\n",
    "```\n",
    "IA (Inteligencia Artificial)\n",
    " ‚îî‚îÄ ML (Machine Learning)\n",
    "     ‚îú‚îÄ Supervisado (con etiquetas)\n",
    "     ‚îÇ   ‚îú‚îÄ Regresi√≥n (output continuo)\n",
    "     ‚îÇ   ‚îî‚îÄ Clasificaci√≥n (output discreto)\n",
    "     ‚îú‚îÄ No Supervisado (sin etiquetas)\n",
    "     ‚îÇ   ‚îú‚îÄ Clustering\n",
    "     ‚îÇ   ‚îú‚îÄ Reducci√≥n de dimensionalidad\n",
    "     ‚îÇ   ‚îî‚îÄ Detecci√≥n de anomal√≠as\n",
    "     ‚îî‚îÄ Por Refuerzo (recompensas)\n",
    "         ‚îî‚îÄ DL (Deep Learning)\n",
    "             ‚îú‚îÄ CNNs (im√°genes)\n",
    "             ‚îú‚îÄ RNNs/LSTM (secuencias)\n",
    "             ‚îî‚îÄ Transformers (NLP)\n",
    "```\n",
    "\n",
    "**Pipeline de ML:**\n",
    "1. Definir problema ‚Üí 2. Recolectar datos ‚Üí 3. EDA ‚Üí 4. Preprocesar ‚Üí 5. Feature Engineering ‚Üí 6. Entrenar modelo ‚Üí 7. Evaluar ‚Üí 8. Optimizar ‚Üí 9. Deployment\n",
    "\n",
    "**Trade-offs:**\n",
    "- **Sesgo vs Varianza**: Underfitting vs Overfitting\n",
    "- **Interpretabilidad vs Performance**: Linear vs XGBoost\n",
    "- **Velocidad vs Precisi√≥n**: KNN vs SVM\n",
    "- **Simplicidad vs Flexibilidad**: Regresi√≥n Lineal vs Neural Network\n",
    "\n",
    "---\n",
    "\n",
    "### **Clase 02: Regresi√≥n**\n",
    "\n",
    "#### F√≥rmulas Clave\n",
    "\n",
    "**Regresi√≥n Lineal:**\n",
    "```\n",
    "≈∑ = Œ≤‚ÇÄ + Œ≤‚ÇÅx‚ÇÅ + Œ≤‚ÇÇx‚ÇÇ + ... + Œ≤‚Çôx‚Çô\n",
    "```\n",
    "\n",
    "**Regularizaci√≥n:**\n",
    "```\n",
    "Ridge (L2):     Loss = MSE + Œª¬∑Œ£Œ≤·µ¢¬≤\n",
    "Lasso (L1):     Loss = MSE + Œª¬∑Œ£|Œ≤·µ¢|\n",
    "ElasticNet:     Loss = MSE + Œª‚ÇÅ¬∑Œ£|Œ≤·µ¢| + Œª‚ÇÇ¬∑Œ£Œ≤·µ¢¬≤\n",
    "```\n",
    "\n",
    "**M√©tricas:**\n",
    "```python\n",
    "MAE  = mean(|y_true - y_pred|)         # En unidades de y\n",
    "RMSE = sqrt(mean((y_true - y_pred)¬≤))  # Penaliza errores grandes\n",
    "R¬≤   = 1 - (SS_res / SS_tot)           # [0, 1], fracci√≥n varianza explicada\n",
    "```\n",
    "\n",
    "**Interpretaci√≥n de R¬≤:**\n",
    "- R¬≤ = 0.50 ‚Üí Modelo explica 50% de la varianza\n",
    "- R¬≤ = 0.70 ‚Üí Bueno\n",
    "- R¬≤ = 0.85 ‚Üí Muy bueno\n",
    "- R¬≤ = 0.95 ‚Üí Excelente (o posible overfitting)\n",
    "- R¬≤ < 0 ‚Üí Modelo peor que predecir la media\n",
    "\n",
    "---\n",
    "\n",
    "### **Clase 03: Regresi√≥n Log√≠stica**\n",
    "\n",
    "#### Funci√≥n Sigmoide\n",
    "\n",
    "```\n",
    "œÉ(z) = 1 / (1 + e^(-z))\n",
    "```\n",
    "\n",
    "Donde z = Œ≤‚ÇÄ + Œ≤‚ÇÅx‚ÇÅ + ... + Œ≤‚Çôx‚Çô\n",
    "\n",
    "**Output**: Probabilidad ‚àà [0, 1]\n",
    "\n",
    "#### Interpretaci√≥n\n",
    "\n",
    "```\n",
    "Odds = P / (1 - P)\n",
    "Log-Odds = ln(Odds) = Œ≤‚ÇÄ + Œ≤‚ÇÅx‚ÇÅ + ... + Œ≤‚Çôx‚Çô\n",
    "Odds Ratio = e^Œ≤\n",
    "```\n",
    "\n",
    "**Ejemplo:**\n",
    "- Si Œ≤‚ÇÅ = 0.5 ‚Üí Odds Ratio = e^0.5 = 1.65\n",
    "- Un aumento de 1 unidad en x‚ÇÅ ‚Üí aumenta odds en 65%\n",
    "\n",
    "#### M√©tricas de Clasificaci√≥n\n",
    "\n",
    "**Matriz de Confusi√≥n:**\n",
    "```\n",
    "                  Predicho\n",
    "               Positivo  Negativo\n",
    "Real Positivo     TP        FN\n",
    "     Negativo     FP        TN\n",
    "```\n",
    "\n",
    "**M√©tricas derivadas:**\n",
    "```\n",
    "Accuracy  = (TP + TN) / Total\n",
    "Precision = TP / (TP + FP)     # \"Cu√°ndo predigo positivo, cu√°ntas veces acierto\"\n",
    "Recall    = TP / (TP + FN)     # \"De todos los positivos reales, cu√°ntos detecto\"\n",
    "F1-Score  = 2¬∑(P¬∑R) / (P+R)    # Media arm√≥nica\n",
    "```\n",
    "\n",
    "**Curva ROC:**\n",
    "- X-axis: FPR = FP / (FP + TN)\n",
    "- Y-axis: TPR = TP / (TP + FN) = Recall\n",
    "- AUC = √Årea bajo la curva [0.5, 1.0]\n",
    "\n",
    "---\n",
    "\n",
    "### **Clase 04: Otros Algoritmos de Clasificaci√≥n**\n",
    "\n",
    "#### K-Nearest Neighbors (KNN)\n",
    "\n",
    "**Idea**: Clasificar seg√∫n k vecinos m√°s cercanos\n",
    "\n",
    "```python\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "knn = KNeighborsClassifier(n_neighbors=5, metric='euclidean')\n",
    "knn.fit(X_train, y_train)\n",
    "```\n",
    "\n",
    "**Ventajas**: Simple, no param√©trico  \n",
    "**Desventajas**: Lento con datos grandes, sensible a escala\n",
    "\n",
    "#### Decision Trees\n",
    "\n",
    "**Criterios de divisi√≥n:**\n",
    "- **Gini impurity**: 1 - Œ£p·µ¢¬≤\n",
    "- **Entropy**: -Œ£p·µ¢¬∑log‚ÇÇ(p·µ¢)\n",
    "\n",
    "```python\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "dt = DecisionTreeClassifier(max_depth=5, min_samples_split=20)\n",
    "dt.fit(X_train, y_train)\n",
    "```\n",
    "\n",
    "**Ventajas**: Interpretable, no requiere normalizaci√≥n  \n",
    "**Desventajas**: Prone a overfitting\n",
    "\n",
    "#### Support Vector Machines (SVM)\n",
    "\n",
    "**Idea**: Encontrar hiperplano que m√°ximiza el margen\n",
    "\n",
    "**Kernels:**\n",
    "- **Linear**: Fronteras lineales\n",
    "- **RBF** (Radial Basis Function): Fronteras no lineales\n",
    "- **Poly**: Polinomial\n",
    "\n",
    "```python\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "svm = SVC(kernel='rbf', C=1.0, gamma='scale')\n",
    "svm.fit(X_train, y_train)\n",
    "```\n",
    "\n",
    "**Hiperpar√°metros:**\n",
    "- **C**: Penalizaci√≥n de errores (menor C = m√°s regularizaci√≥n)\n",
    "- **gamma**: Influencia de un punto (mayor gamma = m√°s overfitting)\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='modulo2'></a>\n",
    "## üîß M√ìDULO 2: MODELOS AVANZADOS (Clases 05-06)\n",
    "\n",
    "### **Clase 05: Modelos de Ensamble**\n",
    "\n",
    "#### Bagging (Bootstrap Aggregating)\n",
    "\n",
    "**Idea**: Entrenar m√∫ltiples modelos con submuestras bootstrapped y promediar\n",
    "\n",
    "```python\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "rf = RandomForestClassifier(\n",
    "    n_estimators=100,      # N√∫mero de √°rboles\n",
    "    max_depth=10,          # Profundidad m√°xima\n",
    "    min_samples_split=20,  # M√≠nimo para dividir\n",
    "    max_features='sqrt',   # Features por split\n",
    "    random_state=42\n",
    ")\n",
    "rf.fit(X_train, y_train)\n",
    "```\n",
    "\n",
    "**Beneficio**: Reduce varianza (menos overfitting)\n",
    "\n",
    "#### Boosting\n",
    "\n",
    "**Idea**: Entrenar modelos secuencialmente, cada uno corrige errores del anterior\n",
    "\n",
    "**XGBoost (El Rey de Kaggle):**\n",
    "```python\n",
    "import xgboost as xgb\n",
    "\n",
    "model = xgb.XGBClassifier(\n",
    "    n_estimators=100,\n",
    "    max_depth=6,\n",
    "    learning_rate=0.1,       # Tasa de aprendizaje\n",
    "    subsample=0.8,           # Fracci√≥n de muestras\n",
    "    colsample_bytree=0.8,    # Fracci√≥n de features\n",
    "    reg_alpha=0.1,           # L1\n",
    "    reg_lambda=1.0,          # L2\n",
    "    random_state=42\n",
    ")\n",
    "model.fit(X_train, y_train)\n",
    "```\n",
    "\n",
    "**Beneficio**: Reduce sesgo (mejor precisi√≥n)\n",
    "\n",
    "**Comparaci√≥n:**\n",
    "\n",
    "| Aspecto | Random Forest (Bagging) | XGBoost (Boosting) |\n",
    "|---------|------------------------|--------------------|\n",
    "| Entrenamiento | Paralelo | Secuencial |\n",
    "| Velocidad | Media | R√°pida |\n",
    "| Overfitting | Robusto | M√°s propenso |\n",
    "| Precisi√≥n | Buena | Excelente |\n",
    "| Cu√°ndo usar | Robustez | M√°xima precisi√≥n |\n",
    "\n",
    "---\n",
    "\n",
    "### **Clase 06: Optimizaci√≥n de Modelos**\n",
    "\n",
    "#### Grid Search\n",
    "\n",
    "```python\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "param_grid = {\n",
    "    'n_estimators': [50, 100, 200],\n",
    "    'max_depth': [3, 5, 10],\n",
    "    'learning_rate': [0.01, 0.1, 0.3]\n",
    "}\n",
    "\n",
    "grid = GridSearchCV(\n",
    "    xgb.XGBClassifier(),\n",
    "    param_grid,\n",
    "    cv=5,\n",
    "    scoring='roc_auc'\n",
    ")\n",
    "grid.fit(X_train, y_train)\n",
    "\n",
    "print(\"Mejor combinaci√≥n:\", grid.best_params_)\n",
    "print(\"Mejor score:\", grid.best_score_)\n",
    "```\n",
    "\n",
    "**Problema**: Muy lento (prueba TODAS las combinaciones)\n",
    "\n",
    "#### Optuna (B√∫squeda Bayesiana)\n",
    "\n",
    "```python\n",
    "import optuna\n",
    "\n",
    "def objective(trial):\n",
    "    params = {\n",
    "        'n_estimators': trial.suggest_int('n_estimators', 50, 300),\n",
    "        'max_depth': trial.suggest_int('max_depth', 3, 10),\n",
    "        'learning_rate': trial.suggest_float('learning_rate', 0.01, 0.3)\n",
    "    }\n",
    "    \n",
    "    model = xgb.XGBClassifier(**params)\n",
    "    scores = cross_val_score(model, X_train, y_train, cv=5, scoring='roc_auc')\n",
    "    return scores.mean()\n",
    "\n",
    "study = optuna.create_study(direction='maximize')\n",
    "study.optimize(objective, n_trials=100)\n",
    "\n",
    "print(\"Mejor combinaci√≥n:\", study.best_params)\n",
    "print(\"Mejor score:\", study.best_value)\n",
    "```\n",
    "\n",
    "**Beneficio**: **10-100x m√°s r√°pido** que Grid Search\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='modulo3'></a>\n",
    "## üîç M√ìDULO 3: APRENDIZAJE NO SUPERVISADO (Clases 07-08)\n",
    "\n",
    "### **Clase 07: Clustering**\n",
    "\n",
    "#### K-Means\n",
    "\n",
    "**Algoritmo:**\n",
    "1. Inicializar k centroides aleatoriamente\n",
    "2. Asignar cada punto al centroide m√°s cercano\n",
    "3. Recalcular centroides (media del cluster)\n",
    "4. Repetir 2-3 hasta convergencia\n",
    "\n",
    "```python\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "kmeans = KMeans(n_clusters=5, random_state=42)\n",
    "labels = kmeans.fit_predict(X)\n",
    "```\n",
    "\n",
    "**Elbow Method** para encontrar k:\n",
    "```python\n",
    "inertias = []\n",
    "for k in range(2, 11):\n",
    "    kmeans = KMeans(n_clusters=k, random_state=42)\n",
    "    kmeans.fit(X)\n",
    "    inertias.append(kmeans.inertia_)\n",
    "\n",
    "plt.plot(range(2, 11), inertias, marker='o')\n",
    "plt.xlabel('N√∫mero de clusters (k)')\n",
    "plt.ylabel('Inertia (SS within clusters)')\n",
    "# Buscar el \"codo\"\n",
    "```\n",
    "\n",
    "#### DBSCAN\n",
    "\n",
    "**Idea**: Clustering basado en densidad\n",
    "\n",
    "```python\n",
    "from sklearn.cluster import DBSCAN\n",
    "\n",
    "dbscan = DBSCAN(eps=0.5, min_samples=5)\n",
    "labels = dbscan.fit_predict(X)\n",
    "\n",
    "# labels = -1 son outliers\n",
    "```\n",
    "\n",
    "**Hiperpar√°metros:**\n",
    "- **eps**: Radio de vecindad\n",
    "- **min_samples**: M√≠nimo de puntos para formar cluster\n",
    "\n",
    "#### PCA (Reducci√≥n de Dimensionalidad)\n",
    "\n",
    "```python\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "pca = PCA(n_components=2)  # Reducir a 2 dimensiones\n",
    "X_reduced = pca.fit_transform(X)\n",
    "\n",
    "print(\"Varianza explicada:\", pca.explained_variance_ratio_)\n",
    "# Ej: [0.45, 0.32] ‚Üí PC1 explica 45%, PC2 explica 32%, total 77%\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### **Clase 08: Sistemas de Recomendaci√≥n**\n",
    "\n",
    "#### Filtrado Colaborativo\n",
    "\n",
    "**User-Based:**\n",
    "```\n",
    "\"Usuarios similares a ti tambi√©n compraron...\"\n",
    "```\n",
    "\n",
    "**Item-Based:**\n",
    "```\n",
    "\"Si te gust√≥ X, tambi√©n te gustar√° Y\"\n",
    "```\n",
    "\n",
    "**Librer√≠a Surprise:**\n",
    "```python\n",
    "from surprise import SVD, Dataset, Reader\n",
    "from surprise.model_selection import cross_validate\n",
    "\n",
    "reader = Reader(rating_scale=(1, 5))\n",
    "data = Dataset.load_from_df(df[['user_id', 'item_id', 'rating']], reader)\n",
    "\n",
    "svd = SVD(n_factors=100, n_epochs=20, lr_all=0.005)\n",
    "cross_validate(svd, data, measures=['RMSE', 'MAE'], cv=5, verbose=True)\n",
    "```\n",
    "\n",
    "#### M√©tricas\n",
    "\n",
    "```python\n",
    "Precision@K = (# relevant in top-K) / K\n",
    "Recall@K = (# relevant in top-K) / (total relevant)\n",
    "NDCG@K = DCG@K / IDCG@K  # Considera orden\n",
    "```\n",
    "\n",
    "**Problema t√≠pico**: Cold start (nuevos usuarios/√≠tems sin historial)\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='modulo4'></a>\n",
    "## üìà M√ìDULO 4: SERIES TEMPORALES Y DEEP LEARNING (Clases 09-10)\n",
    "\n",
    "### **Clase 09: Series Temporales**\n",
    "\n",
    "#### Descomposici√≥n\n",
    "\n",
    "**Modelo Aditivo:**\n",
    "```\n",
    "Y(t) = Tendencia(t) + Estacionalidad(t) + Residuo(t)\n",
    "```\n",
    "\n",
    "**Modelo Multiplicativo:**\n",
    "```\n",
    "Y(t) = Tendencia(t) √ó Estacionalidad(t) √ó Residuo(t)\n",
    "```\n",
    "\n",
    "```python\n",
    "from statsmodels.tsa.seasonal import seasonal_decompose\n",
    "\n",
    "decomposition = seasonal_decompose(\n",
    "    df['value'],\n",
    "    model='additive',\n",
    "    period=7  # Estacionalidad semanal\n",
    ")\n",
    "\n",
    "decomposition.plot()\n",
    "```\n",
    "\n",
    "#### Test de Estacionariedad (ADF)\n",
    "\n",
    "```python\n",
    "from statsmodels.tsa.stattools import adfuller\n",
    "\n",
    "result = adfuller(df['value'])\n",
    "print(f\"ADF Statistic: {result[0]}\")\n",
    "print(f\"P-value: {result[1]}\")\n",
    "\n",
    "if result[1] < 0.05:\n",
    "    print(\"‚úÖ Serie es ESTACIONARIA\")\n",
    "else:\n",
    "    print(\"‚ùå Serie NO es estacionaria ‚Üí Aplicar diferenciaci√≥n\")\n",
    "```\n",
    "\n",
    "#### ARIMA\n",
    "\n",
    "```python\n",
    "from statsmodels.tsa.arima.model import ARIMA\n",
    "\n",
    "model = ARIMA(df['value'], order=(p, d, q))\n",
    "fitted = model.fit()\n",
    "forecast = fitted.forecast(steps=30)  # Predict pr√≥ximos 30 d√≠as\n",
    "```\n",
    "\n",
    "**Par√°metros:**\n",
    "- **p**: Lags pasados (autoregressive)\n",
    "- **d**: Orden de diferenciaci√≥n (para estacionariedad)\n",
    "- **q**: Lags de errores (moving average)\n",
    "\n",
    "#### Forecasting con ML\n",
    "\n",
    "**Transformar a problema supervisado:**\n",
    "\n",
    "```python\n",
    "# Crear lags como features\n",
    "df['lag_1'] = df['value'].shift(1)\n",
    "df['lag_2'] = df['value'].shift(2)\n",
    "df['lag_7'] = df['value'].shift(7)  # Semanal\n",
    "\n",
    "# Features de calendario\n",
    "df['day_of_week'] = df['date'].dt.dayofweek\n",
    "df['month'] = df['date'].dt.month\n",
    "df['is_weekend'] = df['day_of_week'].isin([5,6]).astype(int)\n",
    "\n",
    "# Entrenar XGBoost\n",
    "X = df[['lag_1', 'lag_2', 'lag_7', 'day_of_week', 'month', 'is_weekend']]\n",
    "y = df['value']\n",
    "\n",
    "model = xgb.XGBRegressor(...)\n",
    "model.fit(X_train, y_train)\n",
    "```\n",
    "\n",
    "**‚ö†Ô∏è CR√çTICO**: Validaci√≥n temporal (sin shuffle)\n",
    "\n",
    "---\n",
    "\n",
    "### **Clase 10: Deep Learning con PyTorch**\n",
    "\n",
    "#### Red Neuronal Densa\n",
    "\n",
    "```python\n",
    "import torch\n",
    "from torch import nn\n",
    "\n",
    "class DenseNet(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_classes):\n",
    "        super().__init__()\n",
    "        self.fc1 = nn.Linear(input_size, hidden_size)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.dropout = nn.Dropout(0.2)\n",
    "        self.fc2 = nn.Linear(hidden_size, num_classes)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.fc1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.dropout(x)\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "\n",
    "model = DenseNet(input_size=10, hidden_size=64, num_classes=2)\n",
    "```\n",
    "\n",
    "#### LSTM para Series Temporales\n",
    "\n",
    "```python\n",
    "class LSTM_Model(nn.Module):\n",
    "    def __init__(self, input_size=1, hidden_size=64, num_layers=2):\n",
    "        super().__init__()\n",
    "        self.lstm = nn.LSTM(\n",
    "            input_size=input_size,\n",
    "            hidden_size=hidden_size,\n",
    "            num_layers=num_layers,\n",
    "            batch_first=True,\n",
    "            dropout=0.2\n",
    "        )\n",
    "        self.fc = nn.Linear(hidden_size, 1)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        lstm_out, _ = self.lstm(x)\n",
    "        last_output = lstm_out[:, -1, :]  # √öltimo paso temporal\n",
    "        out = self.fc(last_output)\n",
    "        return out\n",
    "\n",
    "model = LSTM_Model()\n",
    "```\n",
    "\n",
    "#### Training Loop\n",
    "\n",
    "```python\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    model.train()\n",
    "    for X_batch, y_batch in train_loader:\n",
    "        # Forward pass\n",
    "        pred = model(X_batch)\n",
    "        loss = criterion(pred, y_batch)\n",
    "        \n",
    "        # Backward pass\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        \n",
    "        # Update weights\n",
    "        optimizer.step()\n",
    "```\n",
    "\n",
    "#### Resultados Reales (Clase 10 - VIX LSTM)\n",
    "\n",
    "**Dataset**: econotrend_vix_sim.csv\n",
    "- **1,305 observaciones** (5 a√±os, 2020-2025)\n",
    "- **VIX promedio**: 21.54 ¬± 2.34 puntos\n",
    "- **Estacionaria**: ‚úÖ (ADF p-value = 0.000000)\n",
    "- **Autocorrelaci√≥n lag-1**: 0.8784 (muy alta)\n",
    "\n",
    "**Modelo LSTM:**\n",
    "- **Arquitectura**: 2 capas √ó 64 unidades = 50,497 par√°metros\n",
    "- **Lookback**: 10 d√≠as\n",
    "- **Entrenamiento**: 50 √©pocas, 5.47 segundos\n",
    "- **Convergencia**: 91.17% mejora en MSE\n",
    "\n",
    "**Resultados:**\n",
    "- **MAE**: 0.946 puntos VIX\n",
    "- **RMSE**: 1.189 puntos VIX\n",
    "- **R¬≤**: 0.666 (66.6% varianza explicada)\n",
    "- **vs Baseline**: +1.88% mejora (LSTM mejor que persistencia)\n",
    "\n",
    "**Conclusi√≥n real del caso**:\n",
    "- ‚úÖ El LSTM captura patrones (supera baseline)\n",
    "- ‚ö†Ô∏è Mejora modesta (1.88%) ‚Üí VIX tiene componente aleatorio fuerte\n",
    "- ‚úÖ R¬≤ = 0.666 es bueno para series financieras\n",
    "- üéØ El VIX es semi-predecible (66.6% estructura + 33.4% ruido)\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='cheatsheet'></a>\n",
    "## üöÄ CHEAT SHEET DE C√ìDIGO\n",
    "\n",
    "### Pipeline Completo de ML\n",
    "\n",
    "```python\n",
    "# 1. IMPORTACIONES\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import mean_absolute_error, r2_score\n",
    "import xgboost as xgb\n",
    "\n",
    "# 2. CARGA DE DATOS\n",
    "df = pd.read_csv('dataset.csv')\n",
    "\n",
    "# 3. EDA B√ÅSICO\n",
    "print(df.info())\n",
    "print(df.describe())\n",
    "print(df.isnull().sum())\n",
    "\n",
    "# 4. PREPROCESAMIENTO\n",
    "# Manejar faltantes\n",
    "df = df.dropna()  # o fillna()\n",
    "\n",
    "# One-Hot Encoding\n",
    "df = pd.get_dummies(df, columns=['categorical_col'], drop_first=True)\n",
    "\n",
    "# Separar X e y\n",
    "X = df.drop('target', axis=1)\n",
    "y = df['target']\n",
    "\n",
    "# 5. SPLIT\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "# 6. NORMALIZACI√ìN\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)  # NO re-fit\n",
    "\n",
    "# 7. MODELAR\n",
    "model = xgb.XGBRegressor(random_state=42)\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# 8. PREDECIR\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "# 9. EVALUAR\n",
    "mae = mean_absolute_error(y_test, y_pred)\n",
    "r2 = r2_score(y_test, y_pred)\n",
    "print(f\"MAE: {mae:.4f}\")\n",
    "print(f\"R¬≤: {r2:.4f}\")\n",
    "\n",
    "# 10. VALIDACI√ìN CRUZADA\n",
    "scores = cross_val_score(model, X, y, cv=5, scoring='neg_mean_absolute_error')\n",
    "print(f\"CV MAE: {-scores.mean():.4f} ¬± {scores.std():.4f}\")\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### Template de Clasificaci√≥n\n",
    "\n",
    "```python\n",
    "from sklearn.metrics import classification_report, roc_auc_score, confusion_matrix\n",
    "import xgboost as xgb\n",
    "\n",
    "# Modelo\n",
    "model = xgb.XGBClassifier(random_state=42)\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Predicciones\n",
    "y_pred = model.predict(X_test)\n",
    "y_proba = model.predict_proba(X_test)[:, 1]  # Probabilidades clase positiva\n",
    "\n",
    "# M√©tricas\n",
    "print(classification_report(y_test, y_pred))\n",
    "print(f\"ROC-AUC: {roc_auc_score(y_test, y_proba):.4f}\")\n",
    "\n",
    "# Matriz de confusi√≥n\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "sns.heatmap(cm, annot=True, fmt='d')\n",
    "plt.title('Matriz de Confusi√≥n')\n",
    "plt.show()\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### Template de Clustering\n",
    "\n",
    "```python\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import silhouette_score\n",
    "\n",
    "# Normalizar (CR√çTICO para K-Means)\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "# Encontrar k √≥ptimo\n",
    "silhouettes = []\n",
    "for k in range(2, 11):\n",
    "    kmeans = KMeans(n_clusters=k, random_state=42)\n",
    "    labels = kmeans.fit_predict(X_scaled)\n",
    "    silhouettes.append(silhouette_score(X_scaled, labels))\n",
    "\n",
    "# Plot\n",
    "plt.plot(range(2, 11), silhouettes, marker='o')\n",
    "plt.xlabel('k')\n",
    "plt.ylabel('Silhouette Score')\n",
    "plt.show()\n",
    "\n",
    "# Modelo final\n",
    "best_k = 5  # Basado en el gr√°fico\n",
    "kmeans = KMeans(n_clusters=best_k, random_state=42)\n",
    "df['cluster'] = kmeans.fit_predict(X_scaled)\n",
    "\n",
    "# Analizar clusters\n",
    "df.groupby('cluster').mean()\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### Template de LSTM (PyTorch)\n",
    "\n",
    "```python\n",
    "import torch\n",
    "from torch import nn\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "# 1. Normalizar\n",
    "scaler = MinMaxScaler()\n",
    "data_scaled = scaler.fit_transform(data.reshape(-1, 1))\n",
    "\n",
    "# 2. Crear secuencias\n",
    "def create_sequences(data, lookback=10):\n",
    "    X, y = [], []\n",
    "    for i in range(len(data) - lookback):\n",
    "        X.append(data[i:i+lookback])\n",
    "        y.append(data[i+lookback])\n",
    "    return np.array(X), np.array(y)\n",
    "\n",
    "X, y = create_sequences(data_scaled, lookback=10)\n",
    "\n",
    "# 3. Modelo LSTM\n",
    "class LSTM_Model(nn.Module):\n",
    "    def __init__(self, hidden_size=64):\n",
    "        super().__init__()\n",
    "        self.lstm = nn.LSTM(1, hidden_size, 2, batch_first=True, dropout=0.2)\n",
    "        self.fc = nn.Linear(hidden_size, 1)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        lstm_out, _ = self.lstm(x)\n",
    "        return self.fc(lstm_out[:, -1, :])\n",
    "\n",
    "model = LSTM_Model()\n",
    "\n",
    "# 4. Entrenar\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "for epoch in range(50):\n",
    "    model.train()\n",
    "    for X_batch, y_batch in train_loader:\n",
    "        pred = model(X_batch)\n",
    "        loss = criterion(pred, y_batch)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "# 5. Evaluar\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    pred_scaled = model(X_test_tensor).numpy()\n",
    "    pred = scaler.inverse_transform(pred_scaled)\n",
    "    \n",
    "mae = mean_absolute_error(y_test, pred)\n",
    "print(f\"MAE: {mae:.4f}\")\n",
    "```\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='proyecto'></a>\n",
    "## üéØ PROYECTO FINAL: GU√çA PASO A PASO\n",
    "\n",
    "### Fase 1: Selecci√≥n y Exploraci√≥n (2-3 d√≠as)\n",
    "\n",
    "**Tareas:**\n",
    "1. Elegir dataset (Kaggle, UCI, o del curso)\n",
    "2. Definir problema claramente:\n",
    "   - ¬øRegresi√≥n o clasificaci√≥n?\n",
    "   - ¬øCu√°l es la variable objetivo?\n",
    "   - ¬øPor qu√© es importante para el negocio?\n",
    "3. EDA exhaustivo:\n",
    "   - Distribuciones\n",
    "   - Correlaciones\n",
    "   - Outliers\n",
    "   - Valores faltantes\n",
    "\n",
    "**Entregable**: Notebook `01_eda.ipynb`\n",
    "\n",
    "---\n",
    "\n",
    "### Fase 2: Preprocesamiento y Baseline (2 d√≠as)\n",
    "\n",
    "**Tareas:**\n",
    "1. Limpiar datos:\n",
    "   - Manejo de faltantes\n",
    "   - Outliers\n",
    "   - Encoding de categ√≥ricas\n",
    "2. Feature engineering:\n",
    "   - Crear variables derivadas\n",
    "   - Seleccionar features relevantes\n",
    "3. Normalizar/estandarizar\n",
    "4. Implementar modelo baseline simple:\n",
    "   - Regresi√≥n: Linear Regression\n",
    "   - Clasificaci√≥n: Logistic Regression\n",
    "   - Series temporales: Persistencia\n",
    "\n",
    "**Entregable**: Notebooks `02_preprocessing.ipynb` + `03_baseline.ipynb`\n",
    "\n",
    "---\n",
    "\n",
    "### Fase 3: Modelado Avanzado (3-4 d√≠as)\n",
    "\n",
    "**Tareas:**\n",
    "1. Implementar 3+ modelos diferentes:\n",
    "   - Random Forest\n",
    "   - XGBoost/LightGBM\n",
    "   - Otro (SVM, LSTM, etc.)\n",
    "2. Validaci√≥n cruzada para cada uno\n",
    "3. Comparar m√©tricas\n",
    "4. Feature importance\n",
    "\n",
    "**Entregable**: Notebook `04_advanced_models.ipynb`\n",
    "\n",
    "---\n",
    "\n",
    "### Fase 4: Optimizaci√≥n (2 d√≠as)\n",
    "\n",
    "**Tareas:**\n",
    "1. Tuning de hiperpar√°metros (Optuna)\n",
    "2. Ensemble de mejores modelos (opcional)\n",
    "3. Feature selection final\n",
    "4. Evaluaci√≥n en test set\n",
    "\n",
    "**Entregable**: Notebook `05_optimization.ipynb`\n",
    "\n",
    "---\n",
    "\n",
    "### Fase 5: Interpretabilidad y Conclusiones (2 d√≠as)\n",
    "\n",
    "**Tareas:**\n",
    "1. SHAP values (si es posible)\n",
    "2. Feature importance final\n",
    "3. An√°lisis de errores\n",
    "4. Visualizaciones finales\n",
    "5. Conclusiones de negocio\n",
    "6. Limitaciones y pr√≥ximos pasos\n",
    "\n",
    "**Entregable**: Notebook `06_final_evaluation.ipynb`\n",
    "\n",
    "---\n",
    "\n",
    "### Fase 6: Documentaci√≥n (1 d√≠a)\n",
    "\n",
    "**Tareas:**\n",
    "1. README profesional\n",
    "2. requirements.txt actualizado\n",
    "3. Comentarios en c√≥digo\n",
    "4. Presentaci√≥n (opcional)\n",
    "\n",
    "**Entregable**: README.md completo\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ‚ö†Ô∏è ERRORES COMUNES Y C√ìMO EVITARLOS\n",
    "\n",
    "### 1. Data Leakage\n",
    "\n",
    "‚ùå **INCORRECTO**:\n",
    "```python\n",
    "# Normalizar TODO el dataset\n",
    "X_scaled = scaler.fit_transform(X)  # ‚ùå Usa info del test\n",
    "X_train, X_test = train_test_split(X_scaled)\n",
    "```\n",
    "\n",
    "‚úÖ **CORRECTO**:\n",
    "```python\n",
    "# Dividir PRIMERO\n",
    "X_train, X_test = train_test_split(X)\n",
    "\n",
    "# Normalizar despu√©s (fit solo en train)\n",
    "X_train = scaler.fit_transform(X_train)  # ‚úÖ Aprende de train\n",
    "X_test = scaler.transform(X_test)        # ‚úÖ Aplica sin re-aprender\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### 2. Shuffle en Series Temporales\n",
    "\n",
    "‚ùå **INCORRECTO**:\n",
    "```python\n",
    "# Series temporales con shuffle\n",
    "X_train, X_test = train_test_split(X, y, shuffle=True)  # ‚ùå\n",
    "```\n",
    "\n",
    "‚úÖ **CORRECTO**:\n",
    "```python\n",
    "# Respetar orden temporal\n",
    "split_idx = int(len(X) * 0.8)\n",
    "X_train, X_test = X[:split_idx], X[split_idx:]  # ‚úÖ\n",
    "y_train, y_test = y[:split_idx], y[split_idx:]\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### 3. Usar Accuracy en Datos Desbalanceados\n",
    "\n",
    "‚ùå **INCORRECTO**:\n",
    "```python\n",
    "# Fraude: 1% positivos, 99% negativos\n",
    "accuracy = accuracy_score(y_test, y_pred)  # 99% prediciendo todo negativo\n",
    "```\n",
    "\n",
    "‚úÖ **CORRECTO**:\n",
    "```python\n",
    "# Usar m√©tricas apropiadas\n",
    "f1 = f1_score(y_test, y_pred)           # Balance Precision/Recall\n",
    "auc = roc_auc_score(y_test, y_proba)    # Robusto a desbalance\n",
    "print(classification_report(y_test, y_pred))  # Completo\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### 4. Olvidar Baseline\n",
    "\n",
    "‚ùå **INCORRECTO**:\n",
    "```python\n",
    "# Solo entrenar modelo complejo\n",
    "model = xgb.XGBRegressor(...)\n",
    "mae = 5.2\n",
    "print(f\"MAE: {mae}\")  # ¬øEs bueno? No sabemos\n",
    "```\n",
    "\n",
    "‚úÖ **CORRECTO**:\n",
    "```python\n",
    "# Baseline simple primero\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "baseline = LinearRegression()\n",
    "baseline.fit(X_train, y_train)\n",
    "mae_baseline = mean_absolute_error(y_test, baseline.predict(X_test))\n",
    "\n",
    "# Modelo avanzado\n",
    "model = xgb.XGBRegressor(...)\n",
    "model.fit(X_train, y_train)\n",
    "mae_xgb = mean_absolute_error(y_test, model.predict(X_test))\n",
    "\n",
    "# Comparar\n",
    "mejora = ((mae_baseline - mae_xgb) / mae_baseline) * 100\n",
    "print(f\"Baseline MAE: {mae_baseline:.4f}\")\n",
    "print(f\"XGBoost MAE: {mae_xgb:.4f}\")\n",
    "print(f\"Mejora: {mejora:.2f}%\")  # Ahora s√≠ sabemos si es bueno\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### 5. No Hacer Validaci√≥n Cruzada\n",
    "\n",
    "‚ùå **INCORRECTO**:\n",
    "```python\n",
    "# Solo un split\n",
    "X_train, X_test = train_test_split(X, y)\n",
    "model.fit(X_train, y_train)\n",
    "score = model.score(X_test, y_test)  # Suerte de c√≥mo cay√≥ el split\n",
    "```\n",
    "\n",
    "‚úÖ **CORRECTO**:\n",
    "```python\n",
    "# Validaci√≥n cruzada (K-Fold)\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "scores = cross_val_score(model, X, y, cv=5, scoring='r2')\n",
    "print(f\"R¬≤ medio: {scores.mean():.4f} ¬± {scores.std():.4f}\")\n",
    "#           ‚Üë Promedio de 5 splits     ‚Üë Variabilidad\n",
    "```\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üí° TIPS FINALES PARA EL PROYECTO\n",
    "\n",
    "### Qu√© Buscan los Evaluadores\n",
    "\n",
    "1. **EDA Profundo** (no solo df.describe()):\n",
    "   - Distribuciones visualizadas\n",
    "   - Correlaciones analizadas\n",
    "   - Outliers identificados\n",
    "   - Insights de negocio\n",
    "\n",
    "2. **Preprocesamiento Justificado**:\n",
    "   - Por qu√© imputaste faltantes as√≠\n",
    "   - Por qu√© eliminaste outliers (o no)\n",
    "   - Por qu√© elegiste esa normalizaci√≥n\n",
    "\n",
    "3. **M√∫ltiples Modelos Comparados**:\n",
    "   - Al menos 3 diferentes\n",
    "   - Baseline incluido\n",
    "   - Tabla comparativa clara\n",
    "\n",
    "4. **Validaci√≥n Robusta**:\n",
    "   - Cross-validation\n",
    "   - M√©tricas apropiadas\n",
    "   - An√°lisis de errores\n",
    "\n",
    "5. **Interpretabilidad**:\n",
    "   - Feature importance\n",
    "   - Explicaci√≥n de predicciones\n",
    "   - SHAP (bonus)\n",
    "\n",
    "6. **Conclusiones de Negocio**:\n",
    "   - No solo \"R¬≤ = 0.85\"\n",
    "   - Sino \"El modelo predice con 85% de precisi√≥n, permitiendo reducir inventario en 15% y ahorrar $50K/mes\"\n",
    "\n",
    "---\n",
    "\n",
    "### Checklist Pre-Entrega\n",
    "\n",
    "- [ ] README.md profesional con instrucciones de ejecuci√≥n\n",
    "- [ ] requirements.txt actualizado\n",
    "- [ ] Notebooks ejecutan sin errores de principio a fin\n",
    "- [ ] Outputs limpiados (no 1000 l√≠neas de warnings)\n",
    "- [ ] Seeds fijas (reproducibilidad)\n",
    "- [ ] Visualizaciones con t√≠tulos y labels\n",
    "- [ ] Conclusiones al final de cada notebook\n",
    "- [ ] Tabla comparativa de modelos\n",
    "- [ ] Al menos 3 modelos diferentes\n",
    "- [ ] Validaci√≥n cruzada documentada\n",
    "- [ ] Feature importance analizada\n",
    "- [ ] Recomendaciones de negocio espec√≠ficas\n",
    "\n",
    "---\n",
    "\n",
    "### Datasets Sugeridos por Nivel\n",
    "\n",
    "**Principiante:**\n",
    "- Titanic (Kaggle) - Clasificaci√≥n binaria\n",
    "- House Prices (Kaggle) - Regresi√≥n\n",
    "- Iris (UCI) - Clasificaci√≥n multiclase (muy simple)\n",
    "\n",
    "**Intermedio:**\n",
    "- Credit Card Fraud (Kaggle) - Clasificaci√≥n desbalanceada\n",
    "- Bike Sharing (UCI) - Series temporales\n",
    "- Datasets del curso (RetailBoost, CityScoot)\n",
    "\n",
    "**Avanzado:**\n",
    "- Store Sales Forecasting (Kaggle) - Series temporales multivariadas\n",
    "- Santander Customer Satisfaction (Kaggle) - Clasificaci√≥n con feature engineering\n",
    "- Propio dataset de negocio real\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='qa'></a>\n",
    "## ‚ùì Q&A y DUDAS FRECUENTES\n",
    "\n",
    "### Dudas T√©cnicas Comunes\n",
    "\n",
    "**1. \"¬øCu√°ndo usar Random Forest vs XGBoost?\"**\n",
    "\n",
    "**Random Forest** si:\n",
    "- Quieres robustez (menos overfitting)\n",
    "- Tienes poco tiempo para tuning\n",
    "- Interpretabilidad es importante\n",
    "\n",
    "**XGBoost** si:\n",
    "- Necesitas m√°xima precisi√≥n\n",
    "- Est√°s dispuesto a optimizar hyperpar√°metros\n",
    "- Compites en Kaggle üòÑ\n",
    "\n",
    "---\n",
    "\n",
    "**2. \"¬øCu√°ntas features deber√≠a tener mi modelo?\"**\n",
    "\n",
    "**Regla general**:\n",
    "- M√≠nimo: 10 observaciones por feature\n",
    "- Recomendado: 50-100 observaciones por feature\n",
    "- Con 1000 observaciones ‚Üí m√°ximo 100 features (idealmente 20-50)\n",
    "\n",
    "**Si tienes muchas features:**\n",
    "- Feature selection (SelectKBest, RFE)\n",
    "- Regularizaci√≥n L1 (Lasso) ‚Üí auto-selecciona\n",
    "- PCA para reducir\n",
    "\n",
    "---\n",
    "\n",
    "**3. \"¬øC√≥mo s√© si tengo overfitting?\"**\n",
    "\n",
    "**Se√±ales:**\n",
    "- Train score >> Test score (ej: Train R¬≤=0.95, Test R¬≤=0.60)\n",
    "- Gap > 10-15% entre train y test\n",
    "- Modelo muy complejo para pocos datos\n",
    "- Validaci√≥n cruzada muestra alta varianza\n",
    "\n",
    "**Soluciones:**\n",
    "1. M√°s datos (si es posible)\n",
    "2. Regularizaci√≥n (Ridge, Lasso, Dropout)\n",
    "3. Modelo m√°s simple (menos depth, menos features)\n",
    "4. Early stopping (DL)\n",
    "5. Validaci√≥n cruzada + promediar predicciones\n",
    "\n",
    "---\n",
    "\n",
    "**4. \"¬øQu√© m√©trica usar para mi problema?\"**\n",
    "\n",
    "**Regresi√≥n:**\n",
    "- **MAE**: Error promedio f√°cil de interpretar\n",
    "- **RMSE**: Si errores grandes son m√°s costosos\n",
    "- **R¬≤**: Siempre, para ver % varianza explicada\n",
    "\n",
    "**Clasificaci√≥n Balanceada:**\n",
    "- **Accuracy**: OK si clases est√°n balanceadas\n",
    "- **F1-Score**: Balance general\n",
    "\n",
    "**Clasificaci√≥n Desbalanceada:**\n",
    "- **ROC-AUC**: Robusta a desbalance\n",
    "- **F1-Score**: Balance Precision/Recall\n",
    "- **Precision** si minimizar FP es cr√≠tico (spam)\n",
    "- **Recall** si minimizar FN es cr√≠tico (fraude, enfermedades)\n",
    "\n",
    "**Series Temporales:**\n",
    "- **MAE/RMSE**: En unidades del target\n",
    "- **MAPE**: Error porcentual\n",
    "- **Comparar con baseline** (persistencia): Obligatorio\n",
    "\n",
    "---\n",
    "\n",
    "**5. \"Mi R¬≤ es negativo, ¬øqu√© hago?\"**\n",
    "\n",
    "**R¬≤ < 0** significa: Tu modelo es PEOR que predecir la media.\n",
    "\n",
    "**Causas posibles:**\n",
    "1. Modelo muy mal ajustado\n",
    "2. Features no informativas\n",
    "3. Data leakage al rev√©s (test muy diferente a train)\n",
    "4. Bug en c√≥digo\n",
    "\n",
    "**Soluciones:**\n",
    "1. Revisa que X_test y y_test est√©n alineados\n",
    "2. Verifica que el scaler se aplic√≥ correctamente\n",
    "3. Prueba un modelo m√°s simple (LinearRegression)\n",
    "4. Revisa el EDA (¬øhay correlaci√≥n entre X e y?)\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üéì RECURSOS FINALES\n",
    "\n",
    "### Para Consultar Durante el Proyecto\n",
    "\n",
    "**Documentaci√≥n del curso:**\n",
    "- `clase_11_consulta/data/RESUMEN_COMPLETO_CURSADA.md`\n",
    "- Este notebook (impr√≠melo si quieres)\n",
    "\n",
    "**Clase 10 (Ejemplo de proyecto completo):**\n",
    "- `clase_10/notebooks/homework_vix_lstm_completo_didactico.ipynb`\n",
    "- `clase_10/docs/ANALISIS_COMPLETO_VIX_LSTM.md`\n",
    "\n",
    "**Cheat Sheets:**\n",
    "- [Scikit-learn Algorithm Cheat Sheet](https://scikit-learn.org/stable/tutorial/machine_learning_map/)\n",
    "- [Pandas Cheat Sheet](https://pandas.pydata.org/Pandas_Cheat_Sheet.pdf)\n",
    "- [Matplotlib Cheat Sheet](https://matplotlib.org/cheatsheets/)\n",
    "\n",
    "---\n",
    "\n",
    "### Pr√≥ximos Pasos en tu Carrera\n",
    "\n",
    "**Despu√©s de este curso, puedes:**\n",
    "\n",
    "1. **Aplicar a trabajos**:\n",
    "   - Junior Data Scientist\n",
    "   - ML Engineer Jr.\n",
    "   - Data Analyst especializado en ML\n",
    "\n",
    "2. **Continuar aprendiendo**:\n",
    "   - Deep Learning avanzado (CNNs, Transformers)\n",
    "   - MLOps y deployment\n",
    "   - NLP (Procesamiento de Lenguaje Natural)\n",
    "   - Computer Vision\n",
    "\n",
    "3. **Construir portfolio**:\n",
    "   - Kaggle competitions\n",
    "   - Proyectos en GitHub\n",
    "   - Blog t√©cnico\n",
    "   - Contribuir a open source\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üéâ MENSAJE FINAL\n",
    "\n",
    "### Lo que has logrado:\n",
    "\n",
    "‚úÖ **Dominas** 25+ algoritmos de ML  \n",
    "‚úÖ **Conoces** las m√©tricas apropiadas para cada problema  \n",
    "‚úÖ **Puedes** construir un pipeline completo de ML de principio a fin  \n",
    "‚úÖ **Entiendes** cu√°ndo usar cada t√©cnica  \n",
    "‚úÖ **Has practicado** con 20 datasets reales  \n",
    "‚úÖ **Implementaste** modelos en Scikit-learn Y PyTorch  \n",
    "\n",
    "### Para el proyecto final:\n",
    "\n",
    "- **No tengas miedo** de empezar simple (baseline)\n",
    "- **Documenta TODO** (cada decisi√≥n cuenta)\n",
    "- **Compara modelos** (m√≠nimo 3)\n",
    "- **Valida correctamente** (cross-validation)\n",
    "- **Interpreta resultados** (feature importance m√≠nimo)\n",
    "- **Piensa en negocio** (no solo m√©tricas t√©cnicas)\n",
    "\n",
    "### √öltima recomendaci√≥n:\n",
    "\n",
    "**El mejor proyecto NO es el que tiene el R¬≤ m√°s alto.**  \n",
    "**El mejor proyecto es el que est√° bien documentado, bien justificado, y aporta valor de negocio.**\n",
    "\n",
    "---\n",
    "\n",
    "## üìÖ PR√ìXIMA CLASE: CONSULTA\n",
    "\n",
    "**Lunes pr√≥ximo**:  \n",
    "- Trae tus dudas espec√≠ficas\n",
    "- Discutiremos enfoques para proyectos\n",
    "- Resolveremos problemas t√©cnicos\n",
    "- Q&A abierto\n",
    "\n",
    "---\n",
    "\n",
    "## üìß CONTACTO\n",
    "\n",
    "**Instructor**: Mariano Gobea  \n",
    "**Email**: mariano.gobea@mercadolibre.com / gobeamariano@gmail.com\n",
    "\n",
    "---\n",
    "\n",
    "**¬°Mucho √©xito en tu proyecto final! üöÄüìäü§ñ**\n",
    "\n",
    "---\n",
    "\n",
    "*Este notebook es un resumen de las 10 clases. Para profundizar en cualquier tema, vuelve al notebook correspondiente.*"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
