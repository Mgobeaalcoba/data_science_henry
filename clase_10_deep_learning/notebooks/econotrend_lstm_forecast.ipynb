{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ebc932de",
   "metadata": {},
   "source": [
    "# Econotrend — LSTM Forecast (PyTorch)\n",
    "\n",
    "Notebook base para cargar datos, preparar secuencias y entrenar un modelo LSTM para pronóstico del índice VIX simulado.\n",
    "_Archivo de datos esperado_: `econotrend_vix_sim.csv` (columnas: `date`, `vix`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07ed3b17",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Imports\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "print(\"PyTorch version:\", torch.__version__)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1cefbb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Configuración y utilidades\n",
    "DATA_PATH = \"econotrend_vix_sim.csv\"\n",
    "LOOKBACK = 10\n",
    "BATCH_SIZE = 64\n",
    "EPOCHS = 40\n",
    "LR = 1e-3\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "DEVICE\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f6548c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Cargar datos y EDA básico\n",
    "df = pd.read_csv(DATA_PATH, parse_dates=[\"date\"])\n",
    "df = df.sort_values(\"date\").reset_index(drop=True)\n",
    "\n",
    "print(df.head())\n",
    "print(\"\\nResumen:\")\n",
    "print(df.describe())\n",
    "\n",
    "plt.figure(figsize=(10,4))\n",
    "plt.plot(df[\"date\"], df[\"vix\"])\n",
    "plt.title(\"Serie VIX simulada\")\n",
    "plt.xlabel(\"Fecha\")\n",
    "plt.ylabel(\"VIX\")\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Normalización [0,1]\n",
    "scaler = MinMaxScaler(feature_range=(0,1))\n",
    "vix_scaled = scaler.fit_transform(df[[\"vix\"]]).astype(np.float32)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a203c749",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Construcción de ventanas (lookback=10) -> predicción del siguiente día\n",
    "def build_sequences(series, lookback=10):\n",
    "    X, y = [], []\n",
    "    for i in range(len(series) - lookback):\n",
    "        X.append(series[i:i+lookback])\n",
    "        y.append(series[i+lookback])\n",
    "    X = np.array(X)  # (N, lookback, 1)\n",
    "    y = np.array(y)  # (N, 1)\n",
    "    return X, y\n",
    "\n",
    "X, y = build_sequences(vix_scaled, LOOKBACK)\n",
    "print(\"Shapes:\", X.shape, y.shape)\n",
    "\n",
    "# División 80/20 respetando orden temporal\n",
    "split_idx = int(len(X) * 0.8)\n",
    "X_train, X_test = X[:split_idx], X[split_idx:]\n",
    "y_train, y_test = y[:split_idx], y[split_idx:]\n",
    "\n",
    "class SeqDataset(Dataset):\n",
    "    def __init__(self, X, y):\n",
    "        self.X = torch.from_numpy(X).float()\n",
    "        self.y = torch.from_numpy(y).float()\n",
    "    def __len__(self):\n",
    "        return len(self.X)\n",
    "    def __getitem__(self, idx):\n",
    "        return self.X[idx], self.y[idx]\n",
    "\n",
    "train_loader = DataLoader(SeqDataset(X_train, y_train), batch_size=BATCH_SIZE, shuffle=False)\n",
    "test_loader  = DataLoader(SeqDataset(X_test,  y_test ), batch_size=BATCH_SIZE, shuffle=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c32e529f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Definición del modelo LSTM\n",
    "class VIXLSTM(nn.Module):\n",
    "    def __init__(self, input_size=1, hidden_size=64, num_layers=2, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.lstm = nn.LSTM(input_size=input_size, hidden_size=hidden_size, num_layers=num_layers,\n",
    "                            batch_first=True, dropout=dropout)\n",
    "        self.fc = nn.Linear(hidden_size, 1)\n",
    "    def forward(self, x):\n",
    "        out, _ = self.lstm(x)\n",
    "        out = out[:, -1, :]  # último paso temporal\n",
    "        out = self.fc(out)\n",
    "        return out\n",
    "\n",
    "model = VIXLSTM().to(DEVICE)\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=LR)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d3dbaa4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Entrenamiento\n",
    "train_losses = []\n",
    "\n",
    "for epoch in range(1, EPOCHS+1):\n",
    "    model.train()\n",
    "    epoch_loss = 0.0\n",
    "    for xb, yb in train_loader:\n",
    "        xb = xb.to(DEVICE)\n",
    "        yb = yb.to(DEVICE)\n",
    "        optimizer.zero_grad()\n",
    "        pred = model(xb)\n",
    "        loss = criterion(pred, yb)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        epoch_loss += loss.item() * xb.size(0)\n",
    "    epoch_loss /= len(train_loader.dataset)\n",
    "    train_losses.append(epoch_loss)\n",
    "    if epoch % 5 == 0 or epoch == 1:\n",
    "        print(f\"Epoch {epoch:03d} | Train MSE: {epoch_loss:.6f}\")\n",
    "\n",
    "# Curva de pérdidas\n",
    "plt.figure(figsize=(7,4))\n",
    "plt.plot(train_losses)\n",
    "plt.title(\"Curva de entrenamiento (MSE)\")\n",
    "plt.xlabel(\"Época\")\n",
    "plt.ylabel(\"MSE\")\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b2ef2e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Evaluación en test + baseline de persistencia\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    X_test_t = torch.from_numpy(X_test).float().to(DEVICE)\n",
    "    preds_scaled = model(X_test_t).cpu().numpy()\n",
    "\n",
    "# Desescalar\n",
    "y_test_inv = scaler.inverse_transform(y_test)\n",
    "preds_inv = scaler.inverse_transform(preds_scaled)\n",
    "\n",
    "mae = mean_absolute_error(y_test_inv, preds_inv)\n",
    "rmse = mean_squared_error(y_test_inv, preds_inv, squared=False)\n",
    "r2 = r2_score(y_test_inv, preds_inv)\n",
    "\n",
    "# Baseline de persistencia: y_hat = último valor del lookback\n",
    "persist_scaled = X_test[:, -1, :]  # (N, 1)\n",
    "persist_inv = scaler.inverse_transform(persist_scaled)\n",
    "\n",
    "mae_p = mean_absolute_error(y_test_inv, persist_inv)\n",
    "rmse_p = mean_squared_error(y_test_inv, persist_inv, squared=False)\n",
    "r2_p = r2_score(y_test_inv, persist_inv)\n",
    "\n",
    "print(f\"MAE (LSTM): {mae:.4f} | RMSE (LSTM): {rmse:.4f} | R2 (LSTM): {r2:.4f}\")\n",
    "print(f\"MAE (Persistencia): {mae_p:.4f} | RMSE (Persistencia): {rmse_p:.4f} | R2 (Persistencia): {r2_p:.4f}\")\n",
    "\n",
    "# Gráfico comparación real vs predicho en el set de test\n",
    "plt.figure(figsize=(10,4))\n",
    "plt.plot(df[\"date\"].iloc[LOOKBACK+split_idx+1 : LOOKBACK+split_idx+1+len(y_test_inv)], y_test_inv, label=\"Real\")\n",
    "plt.plot(df[\"date\"].iloc[LOOKBACK+split_idx+1 : LOOKBACK+split_idx+1+len(preds_inv)], preds_inv, label=\"Predicho (LSTM)\")\n",
    "plt.title(\"Comparación real vs predicho (Test)\")\n",
    "plt.xlabel(\"Fecha\")\n",
    "plt.ylabel(\"VIX\")\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdee957d",
   "metadata": {},
   "source": [
    "## Reflexión breve\n",
    "- La serie simulada presenta **tendencia suave** y **ruido estacional** (semanal y mensual), además de choques aleatorios.\n",
    "- El LSTM puede capturar parte de las dependencias temporales; sin embargo, en datos financieros reales con alto ruido, el poder predictivo puede acercarse al de un **random walk**. Se recomienda comparar siempre con un baseline fuerte (persistencia) y analizar si las mejoras son **estables** y **estadísticamente significativas**.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "952075be",
   "metadata": {},
   "source": [
    "## Extras (optativo)\n",
    "- Probar una **GRU** o **LSTM bidireccional** (cambiar la clase del modelo).\n",
    "- Añadir **regresores externos** (p. ej., rendimiento del S&P 500 simulado).\n",
    "- Guardar y recargar pesos con `torch.save(model.state_dict(), \"vix_lstm.pt\")` y `model.load_state_dict(torch.load(\"vix_lstm.pt\"))`.\n",
    "- Implementar un **pronóstico multi-step** (roll-forward durante 5 días usando la ventana más reciente + predicciones previas).\n"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
